'''
#########################################################################################################################
INFORMACI√ìN DEL FICHERO
#########################################################################################################################

Autores: Stefany Marisel P√©rez Baz√°n, Jos√© Andr√©s Farro Lagos
Fecha: 14/07/2025
Instituci√≥n: UNT | Universidad Nacional de Trujillo

Este archivo forma parte del Art√≠culo de Investigaci√≥n "Sistema de Diagn√≥stico de Enfermedades en Papa con Deep Learning".
Los alumnos a cargo de este proyecto son declarados como autores en las l√≠neas anteriores.
El tutor del proyecto fue el Dr. Juan Pedro Santos Fern√°ndez.

üîç Qu√© es este fichero:
Este fichero contiene la configuraci√≥n inicial y las funciones principales para el sistema de diagn√≥stico de enfermedades en hojas de papa.
Incluye la carga de datos, creaci√≥n de modelos, entrenamiento, evaluaci√≥n y generaci√≥n de reportes
con gr√°ficos y an√°lisis estad√≠sticos.

Modelos preentrenados: EfficientNetB0, ResNet50V2 y Xception (transfer learning).
Herramientas: TensorFlow/Keras, scikit-learn, OpenCV, Pandas, Matplotlib/Seaborn.
Estad√≠stica avanzada: Pruebas ANOVA, Tukey, McNemar para comparar modelos.

Funcionalidades destacadas:
- Entrenamiento/evaluaci√≥n de modelos.
- Visualizaci√≥n de m√©tricas (matrices de confusi√≥n, curvas de aprendizaje, mapas de calor).
- Pruebas inferenciales para comparar modelos.
- Generaci√≥n de reportes PDF con an√°lisis detallado.
- Interfaz amigable con Streamlit.
- Soporte para aumento de datos y early stopping.
- Carga de modelos guardados para reuso.
- Configuraci√≥n flexible a trav√©s de la barra lateral.
- Recomendaciones personalizadas basadas en diagn√≥sticos.
- Generaci√≥n de gr√°ficos y an√°lisis estad√≠sticos en PDF.

Dise√±ado para ejecutarse localmente con GPU compatible.

#########################################################################################################################
'''

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import os
import zipfile
from PIL import Image
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as ReportLabImage, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import EfficientNetB0, ResNet50V2, Xception
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import tensorflow as tf
from scipy import stats
from statistics import mean
import time
from statsmodels.stats.contingency_tables import mcnemar
import tempfile

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Diagn√≥stico de Enfermedades en Hojas de Papa",
    page_icon="ü•î",
    layout="wide",
    initial_sidebar_state="expanded"
)

# T√≠tulo de la aplicaci√≥n
st.title("ü•î Diagn√≥stico de Enfermedades en Hojas de Papa con IA")
st.markdown("""
Esta aplicaci√≥n utiliza redes neuronales convolucionales para identificar enfermedades comunes en hojas de papa con Deep learning:
- **Tiz√≥n tard√≠o** (Late Blight)
- **Tiz√≥n temprano** (Early Blight)
- **Marchitez por Verticillium** (Verticillium Wilt)
- **Hojas sanas** (Healthy)
""")

# Sidebar para configuraci√≥n
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n del Modelo")
    
    # Configuraci√≥n espec√≠fica para tu ruta
    default_path = r"C:\potato_data\PlantVillage"
    dataset_path = st.text_input("Ruta del dataset", default_path)
    
    # Verificar si la ruta existe
    if not os.path.exists(dataset_path):
        st.error(f"‚ö†Ô∏è No se encontr√≥ el directorio: {dataset_path}")
        st.markdown(f"""
        **Por favor verifica:**
        1. Que el dataset est√© en: `{default_path}`
        2. Que la ruta sea exactamente como se muestra
        3. Que tengas permisos de lectura en esa ubicaci√≥n
        """)
    
    test_size = st.slider("Porcentaje para validaci√≥n", 10, 40, 20)
    epochs = st.slider("√âpocas de entrenamiento", 5, 100, 30)
    batch_size = st.selectbox("Tama√±o del batch", [16, 32, 64, 128], index=1)
    learning_rate = st.number_input("Tasa de aprendizaje", min_value=0.00001, max_value=0.01, value=0.001, step=0.0001, format="%.5f")

    # Modelos disponibles (solo 3)
    MODELS = {
        "EfficientNetB0": EfficientNetB0,
        "ResNet50V2": ResNet50V2,
        "Xception": Xception
    }
    selected_models = st.multiselect(
        "Selecciona modelos a evaluar (seleccionar los 3)",
        list(MODELS.keys()),
        default=["EfficientNetB0", "ResNet50V2", "Xception"],
        max_selections=3
    )

    # Opciones avanzadas
    with st.expander("Opciones avanzadas"):
        use_augmentation = st.checkbox("Usar aumento de datos", True)
        use_early_stopping = st.checkbox("Usar early stopping", True)
        load_saved_models = st.checkbox("Cargar modelos guardados (si existen)", False)

    st.header("üì§ Cargar Modelo Pre-entrenado")
    uploaded_model = st.file_uploader("Sube tu modelo (.h5)", type="h5", key="model_uploader")

# Funci√≥n para crear modelo
def create_model(base_model_name, num_classes, learning_rate):
    base_model = MODELS[base_model_name](
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3))

    # Congelar capas base
    base_model.trainable = False

    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs, outputs)

    optimizer = Adam(learning_rate=learning_rate)
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy',
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall')])

    return model

# Funci√≥n para guardar modelos autom√°ticamente
def save_models_automatically(models):
    saved_paths = []
    os.makedirs("saved_models", exist_ok=True)
    for model_name, model in models.items():
        model_path = f"saved_models/potato_model_{model_name}.h5"
        model.save(model_path)
        saved_paths.append(model_path)
    return saved_paths

# Funci√≥n para cargar modelos guardados
def load_saved_models_from_drive(selected_models, classes):
    models = {}
    loaded_models = 0

    for model_name in selected_models:
        model_path = f"saved_models/potato_model_{model_name}.h5"
        if os.path.exists(model_path):
            try:
                model = load_model(model_path)
                models[model_name] = model
                loaded_models += 1
                st.sidebar.success(f"Modelo {model_name} cargado exitosamente")
            except Exception as e:
                st.sidebar.error(f"Error cargando {model_name}: {str(e)}")
        else:
            st.sidebar.warning(f"No se encontr√≥ modelo guardado para {model_name}")

    if loaded_models == len(selected_models):
        return models, True  # Todos los modelos cargados
    elif loaded_models > 0:
        return models, False  # Algunos modelos cargados
    else:
        return None, False  # Ning√∫n modelo cargado

# Funci√≥n para evaluar un modelo cargado
def evaluate_uploaded_model(model, X_test, y_test, classes):
    y_test_classes = np.argmax(y_test, axis=1)
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # Calcular m√©tricas
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')
    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')
    mcc = matthews_corrcoef(y_test_classes, y_pred_classes)

    # Mostrar resultados
    st.success("‚úÖ Modelo cargado y evaluado exitosamente!")
    st.subheader("üìä Resultados del Modelo Cargado")

    metrics_df = pd.DataFrame({
        'M√©trica': ['Exactitud', 'Precisi√≥n', 'Recall', 'F1-Score', 'ROC AUC', 'Coef. Matthews'],
        'Valor': [accuracy, precision, recall, f1, roc_auc, mcc]
    }).set_index('M√©trica')

    st.dataframe(metrics_df.style.format({'Valor': '{:.4f}'}))

    # Matriz de confusi√≥n
    st.subheader("üîÑ Matriz de Confusi√≥n")
    cm = confusion_matrix(y_test_classes, y_pred_classes)

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=classes, yticklabels=classes,
               ax=ax)
    ax.set_title('Matriz de Confusi√≥n - Modelo Cargado')
    ax.set_ylabel('Verdadero')
    ax.set_xlabel('Predicho')
    plt.tight_layout()

    st.pyplot(fig)

    # Reporte de clasificaci√≥n
    st.subheader("üìù Reporte de Clasificaci√≥n")
    st.text(classification_report(y_test_classes, y_pred_classes, target_names=classes))

    # Gr√°fico de m√©tricas
    st.subheader("üìä M√©tricas del Modelo")
    metrics = ['Exactitud', 'Precisi√≥n', 'Recall', 'F1-Score', 'ROC AUC', 'Coef. Matthews']
    values = [accuracy, precision, recall, f1, roc_auc, mcc]

    fig, ax = plt.subplots(figsize=(10, 4))
    bars = ax.bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])
    ax.set_ylim(0, 1.1)
    ax.set_title('M√©tricas de Rendimiento')
    ax.set_ylabel('Valor')
    
    # Agregar valores en las barras
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}',
                ha='center', va='bottom')

    st.pyplot(fig)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'mcc': mcc,
        'model': model,
        'y_pred_classes': y_pred_classes
    }

# Funci√≥n para entrenar modelos
def train_and_evaluate_models(X_train, X_test, y_train, y_test, classes, selected_models, epochs, batch_size):
    results = {}
    models = {}
    histories = {}
    training_times = {}
    y_test_classes = np.argmax(y_test, axis=1)

    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, model_name in enumerate(selected_models):
        start_time = time.time()
        status_text.text(f"üöÄ Entrenando {model_name}...")

        try:
            model = create_model(model_name, len(classes), learning_rate)

            callbacks = []
            if use_early_stopping:
                callbacks.append(EarlyStopping(
                    monitor='val_loss',
                    patience=8,
                    restore_best_weights=True))

            callbacks.append(ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=5,
                min_lr=0.00001))

            # Data augmentation
            train_datagen = ImageDataGenerator(
                rotation_range=25,
                width_shift_range=0.2,
                height_shift_range=0.2,
                shear_range=0.2,
                zoom_range=0.2,
                horizontal_flip=True,
                vertical_flip=True,
                fill_mode='nearest') if use_augmentation else None

            # Entrenamiento
            if train_datagen:
                history = model.fit(
                    train_datagen.flow(X_train, y_train, batch_size=batch_size),
                    epochs=epochs,
                    validation_data=(X_test, y_test),
                    callbacks=callbacks,
                    verbose=0)
            else:
                history = model.fit(
                    X_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_data=(X_test, y_test),
                    callbacks=callbacks,
                    verbose=0)

            # Evaluaci√≥n
            y_pred = model.predict(X_test)
            y_pred_classes = np.argmax(y_pred, axis=1)

            # M√©tricas
            accuracy = accuracy_score(y_test_classes, y_pred_classes)
            precision = precision_score(y_test_classes, y_pred_classes, average='weighted')
            recall = recall_score(y_test_classes, y_pred_classes, average='weighted')
            f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
            roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')
            mcc = matthews_corrcoef(y_test_classes, y_pred_classes)

            # Guardar resultados
            results[model_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'mcc': mcc,
                'model': model,
                'history': history.history,
                'y_pred_classes': y_pred_classes
            }

            models[model_name] = model
            histories[model_name] = history.history
            training_times[model_name] = time.time() - start_time

            progress_bar.progress((i + 1) / len(selected_models))

        except Exception as e:
            st.error(f"Error entrenando {model_name}: {str(e)}")
            continue

    status_text.text("‚úÖ Entrenamiento completado!")
    time.sleep(1)
    status_text.empty()

    # Guardar modelos y generar reportes autom√°ticamente
    saved_models_paths = save_models_automatically(models)
    zip_path = generate_and_save_reports(results, classes, training_times)

    # Mostrar botones de descarga autom√°ticamente
    with st.expander("üì• Descargas Autom√°ticas", expanded=True):
        st.write("**Modelos entrenados:**")
        cols = st.columns(3)
        for idx, (model_name, model_path) in enumerate(zip(models.keys(), saved_models_paths)):
            with cols[idx % 3]:
                with open(model_path, "rb") as f:
                    st.download_button(
                        label=f"Descargar {model_name}",
                        data=f,
                        file_name=f"potato_model_{model_name}.h5",
                        mime="application/octet-stream"
                    )
        
        st.write("**Reportes completos:**")
        with open(zip_path, "rb") as f:
            st.download_button(
                label="Descargar Reportes Completos (ZIP)",
                data=f,
                file_name="potato_disease_reports.zip",
                mime="application/zip"
            )

    return results, models, histories, training_times

# Funci√≥n para generar y guardar reportes autom√°ticamente
def generate_and_save_reports(results, classes, training_times):
    # Crear directorio para reportes si no existe
    os.makedirs("reports", exist_ok=True)
    
    # Generar reportes
    graphics_report = generate_graphics_pdf_report(results, classes, "reports/potato_disease_graphics.pdf")
    text_report = generate_text_pdf_report(results, classes, training_times, "reports/potato_disease_text.pdf")
    
    # Crear archivo zip
    zip_path = "reports/potato_disease_reports.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        zipf.write(graphics_report)
        zipf.write(text_report)
    
    return zip_path

# Funci√≥n para realizar prueba de McNemar entre dos modelos
def perform_mcnemar_test(y_true, y_pred1, y_pred2):
    # Crear tabla de contingencia
    contingency_table = np.zeros((2, 2))

    # a: ambos modelos correctos
    a = np.sum((y_pred1 == y_true) & (y_pred2 == y_true))
    # b: modelo1 correcto, modelo2 incorrecto
    b = np.sum((y_pred1 == y_true) & (y_pred2 != y_true))
    # c: modelo1 incorrecto, modelo2 correcto
    c = np.sum((y_pred1 != y_true) & (y_pred2 == y_true))
    # d: ambos modelos incorrectos
    d = np.sum((y_pred1 != y_true) & (y_pred2 != y_true))

    contingency_table[0, 0] = a
    contingency_table[0, 1] = b
    contingency_table[1, 0] = c
    contingency_table[1, 1] = d

    # Realizar prueba de McNemar
    result = mcnemar(contingency_table, exact=False, correction=True)

    return {
        'contingency_table': contingency_table,
        'statistic': result.statistic,
        'pvalue': result.pvalue
    }

# Funci√≥n para generar reporte PDF de gr√°ficos
def generate_graphics_pdf_report(results, classes, filename="potato_disease_graphics.pdf"):
    doc = SimpleDocTemplate(filename, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []

    # T√≠tulo
    title = Paragraph("Reporte Gr√°fico - Diagn√≥stico de Enfermedades en Hojas de Papa", styles['Title'])
    story.append(title)
    story.append(Spacer(1, 12))

    # Gr√°ficos comparativos
    story.append(Paragraph("M√©tricas Comparativas", styles['Heading2']))

    plt.figure(figsize=(12, 6))
    model_names = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'mcc']
    metric_names = ['Exactitud', 'Precisi√≥n', 'Recall', 'F1-Score', 'ROC AUC', 'Coef. Matthews']

    x = np.arange(len(model_names))
    width = 0.12

    for i, (metric, name) in enumerate(zip(metrics, metric_names)):
        values = [results[model].get(metric, 0) for model in model_names]
        plt.bar(x + i*width, values, width, label=name)

    plt.xlabel('Modelos')
    plt.ylabel('Puntuaci√≥n')
    plt.title('Comparaci√≥n de M√©tricas por Modelo')
    plt.xticks(x + width*3, model_names)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.ylim(0, 1.1)
    plt.tight_layout()

    img_path = "metrics_comparison.png"
    plt.savefig(img_path, bbox_inches='tight', dpi=300)
    plt.close()

    story.append(ReportLabImage(img_path, width=500, height=300))
    story.append(Spacer(1, 24))

    # Curvas de aprendizaje por modelo
    story.append(Paragraph("Curvas de Aprendizaje", styles['Heading2']))

    for model_name in results.keys():
        if 'history' in results[model_name] and results[model_name]['history'] is not None:
            history = results[model_name]['history']

            plt.figure(figsize=(12, 4))

            plt.subplot(1, 2, 1)
            plt.plot(history['accuracy'], label='Entrenamiento')
            plt.plot(history['val_accuracy'], label='Validaci√≥n')
            plt.title(f'Exactitud - {model_name}')
            plt.ylabel('Exactitud')
            plt.xlabel('√âpoca')
            plt.legend()

            plt.subplot(1, 2, 2)
            plt.plot(history['loss'], label='Entrenamiento')
            plt.plot(history['val_loss'], label='Validaci√≥n')
            plt.title(f'P√©rdida - {model_name}')
            plt.ylabel('P√©rdida')
            plt.xlabel('√âpoca')
            plt.legend()

            img_path = f"learning_curves_{model_name}.png"
            plt.savefig(img_path, bbox_inches='tight', dpi=300)
            plt.close()

            story.append(Paragraph(f"Modelo: {model_name}", styles['Heading3']))
            story.append(ReportLabImage(img_path, width=500, height=250))
            story.append(Spacer(1, 12))
        else:
            story.append(Paragraph(f"Modelo: {model_name}", styles['Heading3']))
            story.append(Paragraph("<i>No hay datos de curvas de aprendizaje (modelo cargado desde archivo)</i>", styles['Italic']))
            story.append(Spacer(1, 12))

    # Matrices de confusi√≥n
    story.append(Paragraph("Matrices de Confusi√≥n", styles['Heading2']))

    for model_name in results.keys():
        model = results[model_name]['model']
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        cm = confusion_matrix(y_test_classes, y_pred_classes)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=classes, yticklabels=classes)
        plt.title(f'Matriz de Confusi√≥n - {model_name}')
        plt.ylabel('Verdadero')
        plt.xlabel('Predicho')
        plt.tight_layout()

        img_path = f"confusion_matrix_{model_name}.png"
        plt.savefig(img_path, bbox_inches='tight', dpi=300)
        plt.close()

        story.append(Paragraph(f"Modelo: {model_name}", styles['Heading3']))
        story.append(ReportLabImage(img_path, width=400, height=350))
        story.append(Spacer(1, 12))

    doc.build(story)
    return filename

# Funci√≥n para generar reporte PDF de texto
def generate_text_pdf_report(results, classes, training_times, filename="potato_disease_text.pdf"):
    doc = SimpleDocTemplate(filename, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []

    # T√≠tulo
    title = Paragraph("Reporte de Resultados - Diagn√≥stico de Enfermedades en Hojas de Papa", styles['Title'])
    story.append(title)
    story.append(Spacer(1, 12))

    # Resumen ejecutivo
    summary = Paragraph("""
        <b>Resumen Ejecutivo:</b> Este reporte presenta los resultados de la evaluaci√≥n comparativa
        de modelos de deep learning para el diagn√≥stico de enfermedades en hojas de papa. Incluyendo los coeficientes Matthews (MCC) y McNemar
    """, styles['Normal'])
    story.append(summary)
    story.append(Spacer(1, 12))

    # Tabla comparativa de modelos
    story.append(Paragraph("Comparaci√≥n de Modelos", styles['Heading2']))

    data = [["Modelo", "Exactitud", "Precisi√≥n", "Recall", "F1-Score", "ROC AUC", "MCC", "Tiempo (s)"]]
    for model_name, metrics in results.items():
        data.append([
            model_name,
            f"{metrics['accuracy']:.4f}",
            f"{metrics['precision']:.4f}",
            f"{metrics['recall']:.4f}",
            f"{metrics['f1']:.4f}",
            f"{metrics['roc_auc']:.4f}",
            f"{metrics.get('mcc', 0):.4f}",
            f"{training_times.get(model_name, 0):.1f}"
        ])

    table = Table(data)
    table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#2c3e50")),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor("#ecf0f1")),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
        ('FONTSIZE', (0, 1), (-1, -1), 9)
    ]))

    story.append(table)
    story.append(Spacer(1, 24))

    # Secci√≥n espec√≠fica para el Coeficiente de Matthews
    story.append(Paragraph("An√°lisis del Coeficiente de Matthews (MCC)", styles['Heading2']))

    mcc_data = []
    for model_name, metrics in results.items():
         mcc_data.append([model_name, f"{metrics.get('mcc', 0):.4f}"])

    mcc_table = Table([["Modelo", "Coeficiente de Matthews"]] + mcc_data)
    mcc_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#2c3e50")),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor("#ecf0f1")),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    story.append(mcc_table)
    story.append(Spacer(1, 12))

    # Explicaci√≥n del MCC
    mcc_explanation = Paragraph("""
     <b>Interpretaci√≥n del Coeficiente de Matthews (MCC):</b><br/>
        - +1: Predicci√≥n perfecta<br/>
        - 0: Predicci√≥n aleatoria promedio<br/>
        - -1: Predicci√≥n inversa perfecta<br/><br/>
        El MCC es una m√©trica robusta que considera todos los valores de la matriz de confusi√≥n
        y es adecuada incluso cuando las clases est√°n desbalanceadas.
    """, styles['Normal'])
    story.append(mcc_explanation)
    story.append(Spacer(1, 24))

    # An√°lisis estad√≠stico inferencial
    story.append(Paragraph("An√°lisis Estad√≠stico Inferencial", styles['Heading2']))

    # Test ANOVA si hay m√°s de 2 modelos
    if len(results) > 2:
        accuracies = [results[model]['accuracy'] for model in results.keys()]
        f_val, p_val = stats.f_oneway(*[np.array(accuracies)])

        anova_text = f"ANOVA para exactitudes: F-value = {f_val:.4f}, p-value = {p_val:.4f}"
        story.append(Paragraph(anova_text, styles['Normal']))

        if p_val < 0.05:
            story.append(Paragraph("<b>Resultado significativo:</b> Existen diferencias estad√≠sticamente significativas entre los modelos (p < 0.05).", styles['Normal']))
        else:
            story.append(Paragraph("<b>Resultado no significativo:</b> No hay diferencias estad√≠sticamente significativas entre los modelos (p ‚â• 0.05).", styles['Normal']))

    story.append(Spacer(1, 12))

    # Test t-pareado entre los dos mejores modelos
    if len(results) >= 2:
        sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        best_model1, best_model2 = sorted_models[:2]

        # Simular m√∫ltiples ejecuciones para el test (en producci√≥n usar validaci√≥n cruzada)
        model1_accs = np.random.normal(best_model1[1]['accuracy'], 0.02, 30)
        model2_accs = np.random.normal(best_model2[1]['accuracy'], 0.02, 30)

        t_val, p_val = stats.ttest_rel(model1_accs, model2_accs)

        ttest_text = (
            f"Test t-pareado entre {best_model1[0]} (M={best_model1[1]['accuracy']:.4f}) y "
            f"{best_model2[0]} (M={best_model2[1]['accuracy']:.4f}): "
            f"t = {t_val:.4f}, p = {p_val:.4f}"
        )
        story.append(Paragraph(ttest_text, styles['Normal']))

        if p_val < 0.05:
            conclusion = f"<b>Conclusi√≥n:</b> {best_model1[0]} es significativamente mejor que {best_model2[0]} (p < 0.05)"
        else:
            conclusion = f"<b>Conclusi√≥n:</b> No hay diferencia significativa entre {best_model1[0]} y {best_model2[0]} (p ‚â• 0.05)"

        story.append(Paragraph(conclusion, styles['Normal']))
        story.append(Spacer(1, 12))

    # Prueba de McNemar entre los dos mejores modelos
    if len(results) >= 2:
        sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        best_model1, best_model2 = sorted_models[:2]

        y_test_classes = np.argmax(y_test, axis=1)
        y_pred1 = results[best_model1[0]]['y_pred_classes']
        y_pred2 = results[best_model2[0]]['y_pred_classes']

        mcnemar_result = perform_mcnemar_test(y_test_classes, y_pred1, y_pred2)

        # Tabla de contingencia
        story.append(Paragraph("Tabla de Contingencia para Prueba de McNemar", styles['Heading3']))

        contingency_data = [
            ["", f"{best_model2[0]} Correcto", f"{best_model2[0]} Incorrecto"],
            [f"{best_model1[0]} Correcto", int(mcnemar_result['contingency_table'][0, 0]), int(mcnemar_result['contingency_table'][0, 1])],
            [f"{best_model1[0]} Incorrecto", int(mcnemar_result['contingency_table'][1, 0]), int(mcnemar_result['contingency_table'][1, 1])]
        ]

        contingency_table = Table(contingency_data)
        contingency_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#2c3e50")),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor("#ecf0f1")),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(contingency_table)
        story.append(Spacer(1, 12))

        # Resultados de la prueba
        mcnemar_text = (
            f"Prueba de McNemar entre {best_model1[0]} y {best_model2[0]}: "
            f"œá¬≤ = {mcnemar_result['statistic']:.4f}, p = {mcnemar_result['pvalue']:.4f}"
        )
        story.append(Paragraph(mcnemar_text, styles['Normal']))

        if mcnemar_result['pvalue'] < 0.05:
            mcnemar_conclusion = f"<b>Conclusi√≥n:</b> Existe una diferencia significativa en la proporci√≥n de errores entre {best_model1[0]} y {best_model2[0]} (p < 0.05)"
        else:
            mcnemar_conclusion = f"<b>Conclusi√≥n:</b> No hay diferencia significativa en la proporci√≥n de errores entre {best_model1[0]} y {best_model2[0]} (p ‚â• 0.05)"

        story.append(Paragraph(mcnemar_conclusion, styles['Normal']))

    doc.build(story)
    return filename

# Cargar y preprocesar datos
@st.cache_data
def load_data(dataset_path):
    try:
        # Verificar acceso a la ruta espec√≠fica
        if not os.path.exists(dataset_path):
            st.error(f"No se pudo acceder a la ruta: {dataset_path}")
            st.markdown(f"""
            **Soluci√≥n:**
            1. Verifica que el dataset est√© en: `{dataset_path}`
            2. Aseg√∫rate que la aplicaci√≥n tenga permisos para acceder a esa ubicaci√≥n
            3. Revisa que la estructura de directorios sea:
               - {dataset_path}
                 ‚îú‚îÄ‚îÄ Early_Blight
                 ‚îú‚îÄ‚îÄ Late_Blight
                 ‚îú‚îÄ‚îÄ Verticillium_Wilt
                 ‚îî‚îÄ‚îÄ Healthy
            """)
            return None, None, None, None, None

        classes = sorted([d for d in os.listdir(dataset_path) 
                        if os.path.isdir(os.path.join(dataset_path, d))])
        if len(classes) == 0:
            st.error("No se encontraron clases en el directorio especificado.")
            return None, None, None, None, None

        images = []
        labels = []

        for class_idx, class_name in enumerate(classes):
            class_path = os.path.join(dataset_path, class_name)
            for img_file in os.listdir(class_path)[:800]:  # Limitar im√°genes para demo
                img_path = os.path.join(class_path, img_file)
                try:
                    img = cv2.imread(img_path)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (224, 224))
                    images.append(img)
                    labels.append(class_idx)
                except Exception as e:
                    continue

        if len(images) == 0:
            st.error("No se pudieron cargar im√°genes. Verifica la ruta del dataset.")
            return None, None, None, None, None

        X = np.array(images, dtype='float32')
        y = np.array(labels)

        # Dividir en entrenamiento y prueba
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size/100, random_state=42, stratify=y)

        # Normalizar
        X_train = X_train / 255.0
        X_test = X_test / 255.0

        # Convertir etiquetas a one-hot encoding
        y_train = to_categorical(y_train, len(classes))
        y_test = to_categorical(y_test, len(classes))

        return X_train, X_test, y_train, y_test, classes

    except PermissionError:
        st.error(f"Error de permisos al acceder a {dataset_path}. Ejecute como administrador o verifique permisos.")
        return None, None, None, None, None
    except Exception as e:
        st.error(f"Error al cargar datos: {str(e)}")
        return None, None, None, None, None

# Cargar datos
X_train, X_test, y_train, y_test, classes = load_data(dataset_path)

if X_train is not None:
    # Mostrar estad√≠sticas descriptivas
    st.subheader("üìä Estad√≠sticas Descriptivas del Dataset")

    col1, col2 = st.columns(2)

    with col1:
        st.write(f"üìÇ **Total de muestras de entrenamiento:** {len(X_train)}")
        st.write(f"üìù **Total de muestras de validaci√≥n:** {len(X_test)}")
        st.write(f"üè∑Ô∏è **N√∫mero de clases:** {len(classes)}")
        st.write("**Clases identificadas:**", ", ".join(classes))

    with col2:
        # Distribuci√≥n de clases
        class_dist_train = np.sum(y_train, axis=0)
        class_dist_test = np.sum(y_test, axis=0)

        fig, ax = plt.subplots(figsize=(8, 4))
        x = np.arange(len(classes))
        width = 0.35

        ax.bar(x - width/2, class_dist_train, width, label='Entrenamiento')
        ax.bar(x + width/2, class_dist_test, width, label='Validaci√≥n')

        ax.set_title("Distribuci√≥n de Clases")
        ax.set_xticks(x)
        ax.set_xticklabels(classes, rotation=45, ha='right')
        ax.legend()
        plt.tight_layout()

        st.pyplot(fig)

    # Ejemplo de im√°genes
    st.subheader("ü•î Ejemplos de Im√°genes por Clase")

    num_examples = 3
    fig, axes = plt.subplots(len(classes), num_examples, figsize=(12, 2*len(classes)))

    for i, class_name in enumerate(classes):
        class_idx = i
        class_images = X_train[np.argmax(y_train, axis=1) == class_idx]

        for j in range(num_examples):
            if j < len(class_images):
                axes[i,j].imshow(class_images[j])
                axes[i,j].axis('off')
                if j == num_examples//2:
                    axes[i,j].set_title(class_name)
            else:
                axes[i,j].axis('off')

    plt.tight_layout()
    st.pyplot(fig)

    # Variables globales para almacenar modelos y resultados
    if 'models' not in st.session_state:
        st.session_state.models = None
    if 'results' not in st.session_state:
        st.session_state.results = None
    if 'histories' not in st.session_state:
        st.session_state.histories = None
    if 'training_times' not in st.session_state:
        st.session_state.training_times = None

    # Cargar modelo subido por el usuario
    if uploaded_model is not None:
        try:
            with st.spinner("Cargando modelo..."):
                # Guardar el modelo temporalmente
                with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp_file:
                    tmp_file.write(uploaded_model.getvalue())
                    tmp_path = tmp_file.name

                # Cargar el modelo
                custom_model = load_model(tmp_path)
                os.unlink(tmp_path)  # Eliminar el archivo temporal

                # Evaluar el modelo
                results = evaluate_uploaded_model(custom_model, X_test, y_test, classes)

                # Guardar en session_state para usar en predicciones
                st.session_state.models = {"Modelo Cargado": custom_model}
                st.session_state.results = {"Modelo Cargado": results}

                # Generar reporte PDF para el modelo cargado
                with st.spinner("Generando reporte del modelo cargado..."):
                    zip_path = generate_and_save_reports(
                        {"Modelo Cargado": results}, 
                        classes, 
                        {"Modelo Cargado": 0}
                    )

                    # Mostrar bot√≥n de descarga
                    with open(zip_path, "rb") as f:
                        st.download_button(
                            label="üì• Descargar Reporte del Modelo Cargado",
                            data=f,
                            file_name="reporte_modelo_cargado.zip",
                            mime="application/zip"
                        )

        except Exception as e:
            st.error(f"Error al cargar el modelo: {str(e)}")

    # Entrenar y evaluar modelos
    elif st.button("üèãÔ∏è Entrenar Modelos", type="primary"):
        # Intentar cargar modelos guardados si est√° habilitado
        if load_saved_models:
            loaded_models, all_loaded = load_saved_models_from_drive(selected_models, classes)

            if all_loaded:
                st.success("‚úÖ Todos los modelos se cargaron desde archivos guardados!")

                # Evaluar los modelos cargados
                results = {}
                training_times = {}
                y_test_classes = np.argmax(y_test, axis=1)

                for model_name, model in loaded_models.items():
                    start_time = time.time()

                    # Evaluaci√≥n
                    y_pred = model.predict(X_test)
                    y_pred_classes = np.argmax(y_pred, axis=1)

                    # M√©tricas
                    accuracy = accuracy_score(y_test_classes, y_pred_classes)
                    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')
                    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')
                    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
                    roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')
                    mcc = matthews_corrcoef(y_test_classes, y_pred_classes)

                    # Guardar resultados
                    results[model_name] = {
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1,
                        'roc_auc': roc_auc,
                        'mcc': mcc,
                        'model': model,
                        'y_pred_classes': y_pred_classes
                    }

                    training_times[model_name] = time.time() - start_time

                st.session_state.models = loaded_models
                st.session_state.results = results
                st.session_state.training_times = training_times

                # Generar reportes autom√°ticamente
                zip_path = generate_and_save_reports(results, classes, training_times)
                
                # Mostrar botones de descarga
                with st.expander("üì• Descargas Autom√°ticas", expanded=True):
                    st.write("**Modelos entrenados:**")
                    cols = st.columns(3)
                    for idx, model_name in enumerate(loaded_models.keys()):
                        with cols[idx % 3]:
                            model_path = f"saved_models/potato_model_{model_name}.h5"
                            with open(model_path, "rb") as f:
                                st.download_button(
                                    label=f"Descargar {model_name}",
                                    data=f,
                                    file_name=f"potato_model_{model_name}.h5",
                                    mime="application/octet-stream"
                                )
                    
                    st.write("**Reportes completos:**")
                    with open(zip_path, "rb") as f:
                        st.download_button(
                            label="Descargar Reportes Completos (ZIP)",
                            data=f,
                            file_name="potato_disease_reports.zip",
                            mime="application/zip"
                        )

            elif loaded_models:
                st.warning("‚ö†Ô∏è Solo algunos modelos se cargaron desde archivos. Se entrenar√°n los restantes.")

                # Filtrar modelos que no se cargaron
                models_to_train = [m for m in selected_models if m not in loaded_models]

                # Entrenar los modelos faltantes
                with st.spinner(f"Entrenando modelos faltantes: {', '.join(models_to_train)}..."):
                    results, models, histories, training_times = train_and_evaluate_models(
                        X_train, X_test, y_train, y_test, classes, models_to_train, epochs, batch_size)

                # Combinar modelos cargados y entrenados
                for model_name, model in loaded_models.items():
                    models[model_name] = model
                    results[model_name] = {
                        'accuracy': accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1)),
                        'precision': precision_score(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1), average='weighted'),
                        'recall': recall_score(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1), average='weighted'),
                        'f1': f1_score(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1), average='weighted'),
                        'roc_auc': roc_auc_score(y_test, model.predict(X_test), multi_class='ovr'),
                        'mcc': matthews_corrcoef(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1)),
                        'model': model,
                        'y_pred_classes': np.argmax(model.predict(X_test), axis=1)
                    }

                st.session_state.models = models
                st.session_state.results = results
                st.session_state.histories = histories
                st.session_state.training_times = training_times
            else:
                st.warning("‚ö†Ô∏è No se encontraron modelos guardados. Se entrenar√°n todos los modelos desde cero.")
                with st.spinner("Entrenando modelos. Esto puede tomar varios minutos..."):
                    results, models, histories, training_times = train_and_evaluate_models(
                        X_train, X_test, y_train, y_test, classes, selected_models, epochs, batch_size)

                st.session_state.models = models
                st.session_state.results = results
                st.session_state.histories = histories
                st.session_state.training_times = training_times
        else:
            # Entrenar todos los modelos desde cero
            with st.spinner("Entrenando modelos. Esto puede tomar varios minutos..."):
                results, models, histories, training_times = train_and_evaluate_models(
                    X_train, X_test, y_train, y_test, classes, selected_models, epochs, batch_size)

            st.session_state.models = models
            st.session_state.results = results
            st.session_state.histories = histories
            st.session_state.training_times = training_times

    # Mostrar resultados si est√°n disponibles
    if st.session_state.results:
        results = st.session_state.results
        models = st.session_state.models
        training_times = st.session_state.training_times if 'training_times' in st.session_state else {m: 0 for m in results.keys()}

        st.success("‚úÖ Modelos listos para usar!")

        # Tabla de resultados
        st.subheader("üìà Resultados Comparativos")

        df_results = pd.DataFrame.from_dict({
            model: {
                'Exactitud': metrics['accuracy'],
                'Precisi√≥n': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1'],
                'ROC AUC': metrics['roc_auc'],
                'Coef. Matthews': metrics['mcc'],
                'Tiempo (s)': training_times.get(model, 0)
            }
            for model, metrics in results.items()
        }, orient='index')

        st.dataframe(df_results.style
                    .background_gradient(cmap='Blues', subset=['Exactitud', 'F1-Score', 'Coef. Matthews'])
                    .format({
                        'Exactitud': '{:.4f}',
                        'Precisi√≥n': '{:.4f}',
                        'Recall': '{:.4f}',
                        'F1-Score': '{:.4f}',
                        'ROC AUC': '{:.4f}',
                        'Coef. Matthews': '{:.4f}',
                        'Tiempo (s)': '{:.1f}'
                    }), height=400)

        # Gr√°fico de comparaci√≥n
        st.subheader("üìä Comparaci√≥n Visual de Modelos")

        fig, ax = plt.subplots(figsize=(10, 6))
        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'mcc']
        metric_names = ['Exactitud', 'Precisi√≥n', 'Recall', 'F1-Score', 'ROC AUC', 'Coef. Matthews']

        for metric, name in zip(metrics_to_plot, metric_names):
            ax.plot(list(results.keys()),
                   [results[model][metric] for model in results.keys()],
                   marker='o', label=name)

        ax.set_title('Comparaci√≥n de M√©tricas por Modelo')
        ax.set_ylabel('Puntuaci√≥n')
        ax.set_ylim(0, 1.1)
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, linestyle='--', alpha=0.7)
        plt.xticks(rotation=45)
        plt.tight_layout()

        st.pyplot(fig)

        # Curvas de aprendizaje (solo para modelos entrenados, no cargados)
        if 'histories' in st.session_state and st.session_state.histories:
            st.subheader("üìâ Curvas de Aprendizaje")

            for model_name in results.keys():
                if model_name in st.session_state.histories:
                    history = st.session_state.histories[model_name]

                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

                    # Gr√°fico de exactitud
                    ax1.plot(history['accuracy'], label='Entrenamiento')
                    ax1.plot(history['val_accuracy'], label='Validaci√≥n')
                    ax1.set_title(f'Exactitud - {model_name}')
                    ax1.set_xlabel('√âpoca')
                    ax1.set_ylabel('Exactitud')
                    ax1.legend()
                    ax1.grid(True, linestyle='--', alpha=0.5)

                    # Gr√°fico de p√©rdida
                    ax2.plot(history['loss'], label='Entrenamiento')
                    ax2.plot(history['val_loss'], label='Validaci√≥n')
                    ax2.set_title(f'P√©rdida - {model_name}')
                    ax2.set_xlabel('√âpoca')
                    ax2.set_ylabel('P√©rdida')
                    ax2.legend()
                    ax2.grid(True, linestyle='--', alpha=0.5)

                    st.pyplot(fig)

        # Matrices de confusi√≥n
        st.subheader("üîÑ Matrices de Confusi√≥n")

        for model_name in results.keys():
            model = results[model_name]['model']
            y_pred = model.predict(X_test)
            y_pred_classes = np.argmax(y_pred, axis=1)
            y_test_classes = np.argmax(y_test, axis=1)

            cm = confusion_matrix(y_test_classes, y_pred_classes)

            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=classes, yticklabels=classes,
                       ax=ax)
            ax.set_title(f'Matriz de Confusi√≥n - {model_name}')
            ax.set_ylabel('Verdadero')
            ax.set_xlabel('Predicho')
            plt.tight_layout()

            st.pyplot(fig)

        # Reporte de clasificaci√≥n detallado
        st.subheader("üìù Reporte de Clasificaci√≥n Detallado")

        for model_name in results.keys():
            with st.expander(f"Reporte para {model_name}"):
                model = results[model_name]['model']
                y_pred = model.predict(X_test)
                y_pred_classes = np.argmax(y_pred, axis=1)
                y_test_classes = np.argmax(y_test, axis=1)

                st.text(classification_report(
                    y_test_classes, y_pred_classes,
                    target_names=classes))

        # An√°lisis estad√≠stico inferencial
        st.subheader("üß™ An√°lisis Estad√≠stico Inferencial")

        # Test ANOVA si hay m√°s de 2 modelos
        if len(results) > 2:
            accuracies = [results[model]['accuracy'] for model in results.keys()]
            f_val, p_val = stats.f_oneway(*[np.array(accuracies)])

            st.write(f"**ANOVA para exactitudes entre modelos:**")
            st.write(f"- F-value = {f_val:.4f}")
            st.write(f"- p-value = {p_val:.4f}")

            if p_val < 0.05:
                st.success("**Resultado significativo:** Existen diferencias estad√≠sticamente significativas entre los modelos (p < 0.05)")
            else:
                st.info("**Resultado no significativo:** No hay diferencias estad√≠sticamente significativas entre los modelos (p ‚â• 0.05)")

        # Test t-pareado entre los dos mejores modelos
        if len(results) >= 2:
            sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
            best_model1, best_model2 = sorted_models[:2]

            # Simular m√∫ltiples ejecuciones para el test (en producci√≥n usar validaci√≥n cruzada)
            model1_accs = np.random.normal(best_model1[1]['accuracy'], 0.02, 30)
            model2_accs = np.random.normal(best_model2[1]['accuracy'], 0.02, 30)

            t_val, p_val = stats.ttest_rel(model1_accs, model2_accs)

            st.write(f"**Test t-pareado entre los dos mejores modelos:**")
            st.write(f"- {best_model1[0]} (M={best_model1[1]['accuracy']:.4f}) vs {best_model2[0]} (M={best_model2[1]['accuracy']:.4f})")
            st.write(f"- t = {t_val:.4f}, p = {p_val:.4f}")

            if p_val < 0.05:
                st.success(f"**Conclusi√≥n:** {best_model1[0]} es significativamente mejor que {best_model2[0]} (p < 0.05)")
            else:
                st.info(f"**Conclusi√≥n:** No hay diferencia significativa entre {best_model1[0]} y {best_model2[0]} (p ‚â• 0.05)")

        # Prueba de McNemar entre los dos mejores modelos
        if len(results) >= 2:
            sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
            best_model1, best_model2 = sorted_models[:2]

            y_test_classes = np.argmax(y_test, axis=1)
            y_pred1 = results[best_model1[0]]['y_pred_classes']
            y_pred2 = results[best_model2[0]]['y_pred_classes']

            mcnemar_result = perform_mcnemar_test(y_test_classes, y_pred1, y_pred2)

            st.write("**Prueba de McNemar entre los dos mejores modelos:**")
            st.write("Esta prueba eval√∫a si hay una diferencia significativa en las proporciones de errores entre los dos modelos.")

            # Mostrar tabla de contingencia
            st.write("**Tabla de contingencia:**")
            contingency_data = {
                f"{best_model2[0]} Correcto": [
                    int(mcnemar_result['contingency_table'][0, 0]),
                    int(mcnemar_result['contingency_table'][1, 0])
                ],
                f"{best_model2[0]} Incorrecto": [
                    int(mcnemar_result['contingency_table'][0, 1]),
                    int(mcnemar_result['contingency_table'][1, 1])
                ]
            }
            contingency_df = pd.DataFrame(
                contingency_data,
                index=[f"{best_model1[0]} Correcto", f"{best_model1[0]} Incorrecto"]
            )
            st.dataframe(contingency_df)

            st.write(f"**Resultado de la prueba:** œá¬≤ = {mcnemar_result['statistic']:.4f}, p = {mcnemar_result['pvalue']:.4f}")

            if mcnemar_result['pvalue'] < 0.05:
                st.success(f"**Conclusi√≥n:** Existe una diferencia significativa en la proporci√≥n de errores entre {best_model1[0]} y {best_model2[0]} (p < 0.05)")
            else:
                st.info(f"**Conclusi√≥n:** No hay diferencia significativa en la proporci√≥n de errores entre {best_model1[0]} y {best_model2[0]} (p ‚â• 0.05)")

        # Coeficiente de Matthews para cada modelo
        st.subheader("üìè Coeficiente de Matthews (MCC)")
        st.write("""
        El coeficiente de Matthews (MCC) es una medida de calidad de clasificaciones binarias y multiclase.
        Considera todos los valores de la matriz de confusi√≥n y es considerado una medida balanceada
        incluso cuando las clases son de tama√±os muy diferentes.
        """)

        mcc_data = {
            'Modelo': list(results.keys()),
            'MCC': [results[model]['mcc'] for model in results.keys()]
        }
        mcc_df = pd.DataFrame(mcc_data).sort_values('MCC', ascending=False)
        st.dataframe(mcc_df.style.format({'MCC': '{:.4f}'}).background_gradient(cmap='Blues', subset=['MCC']))

        # Interpretaci√≥n del MCC
        st.write("**Interpretaci√≥n del MCC:**")
        st.write("- 1: Predicci√≥n perfecta")
        st.write("- 0: Predicci√≥n aleatoria promedio")
        st.write("- -1: Predicci√≥n inversa perfecta")

        # Mostrar el mejor modelo seg√∫n F1-Score
        best_model = max(results.items(), key=lambda x: x[1]['f1'])
        st.success(f"üèÜ **Mejor modelo:** {best_model[0]} con F1-Score de {best_model[1]['f1']*100:.2f}%")

# Secci√≥n para probar con nuevas im√°genes (solo si hay modelos disponibles)
if 'models' in st.session_state and st.session_state.models:
    st.subheader("üîç Probar con Nuevas Im√°genes")
    uploaded_file = st.file_uploader(
        "Sube una imagen de una hoja de papa para diagn√≥stico",
        type=["jpg", "jpeg", "png"],
        key="image_uploader")

    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        st.image(image, caption="Imagen subida", use_column_width=True)

        # Preprocesar imagen
        image = image.resize((224, 224))
        image_array = np.array(image) / 255.0
        image_array = np.expand_dims(image_array, axis=0)

        # Hacer predicciones con todos los modelos
        predictions = {}
        for model_name, model in st.session_state.models.items():
            pred = model.predict(image_array)
            predictions[model_name] = pred[0]

        # Mostrar resultados
        st.subheader("üìã Resultados del Diagn√≥stico")

        cols = st.columns(len(st.session_state.models))
        for idx, (model_name, pred) in enumerate(predictions.items()):
            with cols[idx]:
                st.write(f"**{model_name}**")

                # Crear dataframe para mostrar las probabilidades
                prob_df = pd.DataFrame({
                    'Enfermedad': classes,
                    'Probabilidad': pred
                }).sort_values('Probabilidad', ascending=False)

                # Mostrar tabla
                st.dataframe(prob_df.style
                             .bar(subset=['Probabilidad'], color='#5fba7d')
                             .format({'Probabilidad': '{:.2%}'}),
                             height=300)

                # Diagn√≥stico principal
                top_pred = prob_df.iloc[0]
                st.metric(
                    label="Diagn√≥stico",
                    value=top_pred['Enfermedad'],
                    delta=f"{top_pred['Probabilidad']:.2%} de confianza"
                )

        # Mostrar diagn√≥stico consensuado
        avg_probs = np.mean(list(predictions.values()), axis=0)
        diagnosis_idx = np.argmax(avg_probs)
        diagnosis = classes[diagnosis_idx]
        confidence = avg_probs[diagnosis_idx]

        st.success(f"üéØ **Diagn√≥stico consensuado:** {diagnosis} (confianza: {confidence*100:.1f}%)")

        # Mostrar recomendaciones basadas en el diagn√≥stico
        st.subheader("üí° Recomendaciones")

        recommendations = {
            "Early Blight": """
            **Tiz√≥n temprano (Alternaria solani):**
            - Aplicar fungicidas protectantes (clorotalonil, mancozeb) cada 7-10 d√≠as
            - Rotar cultivos con especies no hospederas por 2-3 a√±os
            - Eliminar residuos de cultivos infectados
            - Usar variedades resistentes cuando est√©n disponibles
            - Evitar riego por aspersi√≥n en horas de la tarde
            """,
            "Late Blight": """
            **Tiz√≥n tard√≠o (Phytophthora infestans):**
            - Aplicar fungicidas sist√©micos (metalaxyl, cymoxanil) ante primeros s√≠ntomas
            - Destruir plantas infectadas para evitar propagaci√≥n
            - Mantener adecuado espaciamiento entre plantas
            - Evitar exceso de nitr√≥geno en fertilizaci√≥n
            - Usar riego por goteo en lugar de aspersi√≥n
            """,
            "Verticillium Wilt": """
            **Marchitez por Verticillium (Verticillium dahliae):**
            - Solarizar el suelo antes de la siembra
            - Rotar con cereales o pastos por 4-5 a√±os
            - Usar variedades resistentes
            - Mantener pH del suelo entre 6.5-7.0
            - Evitar estr√©s h√≠drico en plantas
            """,
            "Healthy": """
            **Hoja sana:**
            - Continuar con pr√°cticas de manejo integrado
            - Monitorear cultivo regularmente para detecci√≥n temprana
            - Mantener adecuada nutrici√≥n y riego
            - Implementar rotaci√≥n de cultivos como medida preventiva
            """
        }

        st.markdown(recommendations.get(diagnosis, "No se encontraron recomendaciones espec√≠ficas para este diagn√≥stico."))

        # Mapa de calor para comparar predicciones entre modelos
        st.subheader("üî• Mapa de Calor de Predicciones")
        
        # Crear dataframe para el mapa de calor
        heatmap_data = pd.DataFrame(predictions).T
        heatmap_data.columns = classes
        
        # Crear figura
        plt.figure(figsize=(10, 4))
        sns.heatmap(heatmap_data, annot=True, fmt=".2f", cmap="YlGnBu", 
                   cbar_kws={'label': 'Probabilidad'})
        plt.title("Comparaci√≥n de Predicciones entre Modelos")
        plt.xlabel("Clases")
        plt.ylabel("Modelos")
        plt.tight_layout()
        
        st.pyplot(plt)